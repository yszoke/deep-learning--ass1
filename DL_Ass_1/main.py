import os
import time

import numpy as np


def initialize_parameters(layer_dims):
    """
    :param layer_dims: an array of the dimensions of each layer in the network
    (layer 0 is the size of the flattened input, layer L is the output softmax)
    :return: a dictionary containing the initialized W and b parameters of
    each layer (W1…WL, b1…bL).
    """

    # run a for loop (1->layer_dims length) in each iteration init the
    # weights (Wi) with np random and bias (Bi) with np zeros
    parameters = {}
    for current_layer in range(1, len(layer_dims)):
        parameters[current_layer] = [np.random.rand(layer_dims[current_layer-1]*layer_dims[current_layer]),
                                     np.zeros(layer_dims[current_layer])]

    return parameters


def linear_forward(A, W, b):
    """
    Implement the linear part of a layer's forward propagation.
    :param A: the activations of the previous layer
    :param W:the weight matrix of the current layer (of shape
    [size of current layer, size of previous layer])
    :param b: the bias vector of the current layer
    (of shape [size of current layer, 1])
    :return:
    Z – the linear component of the activation function (i.e., the value before
    applying the non-linear function)
    linear_cache – a dictionary containing A, W, b
    (stored for making the backpropagation easier to compute)
    """

    # Z = (dot product (vector multiplication) of array input A and
    # array weights W )+ bias B
    # do not forget to return also the cache dictionary A W B
    Z = np.dot(A, W)
    return Z + b, (A, W, b)


def softmax(Z):
    """
    :param Z: the linear component of the activation function
    :return: A - the activations of the layer
    activation_cache – returns Z, which will be useful for the backpropagation

    # implement softmax activation function -
    # https://stackoverflow.com/questions/34968722/how-to-implement-the-softmax-function-in-python
    # do not forget to return activation cache
    # = e ^ x / sum(e ^ x)
    """
    e_x = np.exp(np.subtract(Z, np.max(Z, axis=0)))
    return e_x / e_x.sum(axis=0), Z


def relu(Z):
    """
     :param Z: the linear component of the activation function
     :return: A - the activations of the layer
     activation_cache – returns Z, which will be useful for the backpropagation
     """

    # implement relu function
    # if Z <0 -> return 0 else return Z.
    # do not forget to return activation cache
    A = 0 if Z < 0 else Z
    return A, Z


def linear_activation_forward(A_prev, W, B, activation):
    """
    Implement the forward propagation for the LINEAR->ACTIVATION layer
    :param A_prev: activations of the previous layer
    :param W: the weights matrix of the current layer
    :param B: the bias vector of the current layer
    :param activation: the activation function to be used
    (a string, either “softmax” or “relu”)
    :return:
    A – the activations of the current layer
    cache – a joint dictionary containing both linear_cache and activation_cache

    # First step - use linear forward function (input is A_prev,W,B)
    # Then use an if statement to chose the activation function (relu/ softmax)
    # return cache and A - the activation of the current layer
    """
    Z, linear_cache = linear_forward(A_prev, W, B)
    if activation == 'relu':
        A, activation_cache = relu(Z)
    elif activation == 'softmax':
        A, activation_cache = softmax(Z)
    return A, [linear_cache, activation_cache]


def L_model_forward(X, parameters, use_batchnorm):
    """
    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX
    computation
    :param X: the data, numpy array of shape (input size, number of examples)
    :param parameters: the initialized W and b parameters of each layer
    :param use_batchnorm: a boolean flag used to determine whether to apply
    batchnorm after the activation (note that this option needs to be set to “false” in Section 3 and “true” in Section 4).
    :return:
    AL – the last post-activation value
    caches – a list of all the cache objects generated by the linear_forward function

    # use linear_activation_forward function with relu activation function in an
    iterative way (iteration for each layer starting with the input layer X and
    ending with 1 layer before the last layer)
    # for the last layer - use linear_activation_forward with softmax
    activation function.
    """
    num_of_layers = len(parameters)
    A = X
    cache_list = []
    for layer in range(1, num_of_layers):
        W = parameters[layer][0]
        B = parameters[layer][1]
        A_prev = A
        A, cache = linear_activation_forward(A_prev, W, B, 'relu')
        if use_batchnorm:
            A = apply_batchnorm(A)
        cache_list.append(cache)

    W = parameters[num_of_layers][0]
    B = parameters[num_of_layers][1]
    A_prev = A
    A, cache = linear_activation_forward(A_prev, W, B, 'softmax')
    cache_list.append(cache)
    return A, cache_list


def compute_cost(AL, Y):
    """
    Implement the cost function defined by equation.
    :param AL: probability vector corresponding to your label predictions,
    shape (num_of_classes, number of examples)
    :param Y: the labels vector (i.e. the ground truth).
    :return: cost – the cross-entropy cost.
    """

    #implement cross entropy cost function: p=Y, q=AL
    log_AL = np.nan_to_num(np.log(AL))
    multiple = np.nan_to_num(np.multiply(Y, log_AL))
    return np.nan_to_num(-np.sum(multiple) / AL.shape[1])


def apply_batchnorm(A):
    """
    performs batchnorm on the received activation values of a given layer.
    :param A: the activation values of a given layer
    :return: NA - the normalized activation values, based on the formula
    learned in class
    """

    # compute sum, mean, std and epsilon
    sum = np.sum(A, axis=1)
    mean = sum / A.shape[1]
    var = np.sum(np.square(A-mean), axis=1) / A.shape[1]
    epsilon = np.finfo(float).eps
    # use the following function: (A-mean)/ sqrt(var+epsilon)
    return np.divide(np.subtract(A, mean), np.sqrt(var+epsilon))

# backpropagation


def Linear_backward(dZ, cache):
    """
    Implements the linear part of the backward propagation process for a
    single layer
    :param dZ: the gradient of the cost with respect to the linear output of
    the current layer (layer l)
    :param cache: tuple of values (A_prev, W, b) coming from the forward
    propagation in the current layer
    :return:
    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    dW -- Gradient of the cost with respect to W (current layer l), same shape as W
    db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """
    pass


def linear_activation_backward(dA, cache, activation):
    """
    Implements the backward propagation for the LINEAR->ACTIVATION layer. The
    function first computes dZ and then applies the linear_backward function.
    :param dA: post activation gradient of the current layer
    :param cache: contains both the linear cache and the activations cache
    :param activation: the activation function used (relu/softmax)
    :return:
    dA_prev – Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    dW – Gradient of the cost with respect to W (current layer l), same shape as W
    db – Gradient of the cost with respect to b (current layer l), same shape as b
    """
    pass


def relu_backward(dA, activation_cache):
    """
    Implements backward propagation for a ReLU unit
    :param dA: the post-activation gradient
    :param activation_cache: contains Z (stored during the forward propagation)
    :return: gradient of the cost with respect to Z
    """
    pass


def softmax_backward(dA, activation_cache):
    """
    Implements backward propagation for a softmax unit
    :param dA: the post-activation gradient
    :param activation_cache: contains Z (stored during the forward propagation)
    :return: dZ – gradient of the cost with respect to Z
    """
    pass


def L_model_backward(AL, Y, caches):
    """
    Implement the backward propagation process for the entire network.
    :param AL: - the probabilities vector, the output of the forward propagation
     (L_model_forward)
    :param Y: the true labels vector (the "ground truth" - true classifications)
    :param caches: list of caches containing for each layer:
    a) the linear cache;
    b) the activation cache
    :return:
    Grads - a dictionary with the gradients
             grads["dA" + str(l)] = ...
             grads["dW" + str(l)] = ...
             grads["db" + str(l)] = ...
    """
    pass


def Update_parameters(parameters, grads, learning_rate):
    """
    Updates parameters using gradient descent
    :param parameters: a python dictionary containing the DNN architecture’s
    parameters
    :param grads: a python dictionary containing the gradients
    (generated by L_model_backward)
    :param learning_rate: the learning rate used to update the parameters
    (the “alpha”)
    :return: parameters – the updated values of the parameters object
    provided as input
    """
    pass


# Section 3 train a model using the previous functions
def L_layer_model(X, Y, layers_dims, learning_rate, num_iterations, batch_size):
    """
    Implements a L-layer neural network. All layers but the last should have the
    ReLU activation function, and the final layer will apply the softmax
    activation function. The size of the output layer should be equal to the
    number of labels in the data. Please select a batch size that enables your
    code to run well (i.e. no memory overflows while still running
    relatively fast).
    :param X: the input data, a numpy array of shape
    (height*width , number_of_examples)
    :param Y: the “real” labels of the data, a vector of shape
    (num_of_classes, number of examples)
    :param layers_dims: a list containing the dimensions of each layer,
    including the input
    :param learning_rate: the value to "jump" in every gradient decent iteration
    :param num_iterations: the total number of iteration.
    :param batch_size: the number of examples in a single training batch.
    :return:
    the parameters learnt by the system during the training
    (the same parameters that were updated in the update_parameters function).
    the values of the cost function (calculated by the compute_cost function).
    One value is to be saved after each 100 training iterations (e.g. 3000 iterations -> 30 values).
    """
    pass


def Predict(X, Y, parameters):
    """
    The function receives an input data and the true labels and calculates
    the accuracy of the trained neural network on the data.
    :param X: the input data, a numpy array of shape (height*width, number_of_examples)
    :param Y: the “real” labels of the data, a vector of shape
    (num_of_classes, number of examples)
    :param parameters: a python dictionary containing the DNN
    architecture’s parameters
    :return:
    accuracy – the accuracy measure of the neural net on the provided data
    (i.e. the percentage of the samples for which the correct label receives the
    highest confidence score). Use the softmax function to normalize the
    output values.
    """
    pass


if __name__ == '__main__':
    ## print(initialize_parameters([3, 2, 2]))
    ## print(linear_forward([0.5, 0.2], [0.7, 0.3], 0.8))
    print(softmax([0.8, 0.4, 0.5]))