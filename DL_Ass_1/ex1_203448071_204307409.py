# Amit Giloni ID 203448071, Nimrod Cohen ID 204307409
import numpy as np
import pandas as pd
import sys
import time
from tensorflow.keras.datasets import mnist
from keras.utils import np_utils
from sklearn.model_selection import train_test_split


def initialize_parameters(layer_dims):
    """
    :param layer_dims: an array of the dimensions of each layer in the network (layer 0 is the size of the flattened
    input, layer L is the output softmax)
    :return: a dictionary containing the initialized W and b parameters of each layer (W1…WL, b1…bL).
    """
    output_dict = {}
    for layer_num in range(1, layer_dims.size):
        wb_num = layer_dims.item(layer_num)
        privios_wb_num = layer_dims.item(layer_num-1)
        w_values = np.random.rand(wb_num, privios_wb_num) * 0.01
        while np.any(np.all(w_values == w_values[0,:], axis = 0)):   # Ensure each w value is different
            w_values = np.random.rand(wb_num, privios_wb_num)
        output_dict.update({"W{}".format(layer_num): w_values})
        output_dict.update({"b{}".format(layer_num): np.zeros((wb_num,1))})
    return output_dict


def linear_forward(A, W, b):
    """
    Implement the linear part of a layer's forward propagation.
    :param A: the activations of the previous layer
    :param W: the weight matrix of the current layer (of shape [size of current layer, size of previous layer])
    :param b: the bias vector of the current layer (of shape [size of current layer, 1])
    :return: Z – the linear component of the activation function (i.e., the value before applying the non-linear
            function)
            linear_cache – a dictionary containing A, W, b (stored for making the backpropagation easier to compute)
    """
    Z = np.dot(W, A) + b
    linear_cache = {'A': A, 'W': W, 'b': b}
    return Z, linear_cache


def softmax(Z):
    """
    :param Z: the linear component of the activation function
    :return: A – the activations of the layer
            activation_cache – returns Z, which will be useful for the backpropagation
    """
    e_power = np.exp(np.subtract(Z, np.max(Z, axis=0)))
    e_sum = np.nan_to_num(np.sum(e_power, axis=0))
    A = e_power / e_sum
    activation_cache = Z
    return A, activation_cache


def relu(Z):
    """
    :param Z: the linear component of the activation function
    :return: A – the activations of the layer
            activation_cache – returns Z, which will be useful for the backpropagation
    """
    A = Z.clip(0)
    activation_cache = Z
    return A, activation_cache


def linear_activation_forward(A_prev, W, B, activation, dropout):
    """
    Implement the forward propagation for the LINEAR->ACTIVATION layer
    :param A_prev: activations of the previous layer
    :param W: the weights matrix of the current layer
    :param B: the bias vector of the current layer
    :param activation: the activation function to be used (a string, either “softmax” or “relu”)
    :param dropout: the probability of keeping an hidden unit after the activation
    :return: A – the activations of the current layer
            cache – a joint dictionary containing both linear_cache, activation_cache and dropout_cache
    """
    cache = {}
    linear, linear_cache = linear_forward(A_prev, W, B)
    if activation == 'softmax':
        A, activation_cache = softmax(linear)
    else:
        A, activation_cache = relu(linear)
        if dropout < 1:
            drop_matrix = np.random.rand(A.shape[0], A.shape[1])
            drop_matrix = (drop_matrix < dropout)
            A = np.multiply(A, drop_matrix)
            A = np.divide(A, dropout)
            cache.update({"dropout_cache": drop_matrix})
    cache.update({"activation_cache": activation_cache})
    cache.update({"linear_cache": linear_cache})
    return A, cache


def L_model_forward(X, parameters, use_batchnorm, dropout):
    """
    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation
    :param X: the data, numpy array of shape (input size, number of examples)
    :param parameters: the initialized W and b parameters of each layer
    :param use_batchnorm: a boolean flag used to determine whether to apply batchnorm after the activation
            (note that this option needs to be set to “false” in Section 3 and “true” in Section 4).
    :param dropout: the probability of keeping an hidden unit after the activation
    :return: AL – the last post-activation value
            caches – a list of all the cache objects generated by the linear_forward function
    """
    caches = []
    layer_a = X
    hidden_num = len(parameters) // 2
    for layer_num in range(1, hidden_num):
        A_prev = layer_a.copy()
        layer_a, layer_cache = linear_activation_forward(A_prev, parameters.get("W{}".format(layer_num)),
                                                         parameters.get("b{}".format(layer_num)), 'relu', dropout)
        if use_batchnorm:
            layer_a = apply_batchnorm(layer_a)
            layer_cache.update({'activation_cache':layer_a})
        caches.append(layer_cache)
    AL, al_cache = linear_activation_forward(layer_a, parameters.get("W{}".format(hidden_num)),
                                                    parameters.get("b{}".format(hidden_num)), 'softmax', dropout)
    caches.append(al_cache)
    return AL, caches


def compute_cost(AL, Y):
    """
    :param AL: probability vector corresponding to your label predictions, shape(num_of_classes, number of examples)
    :param Y: the labels vector(i.e.the ground truth)
    :return: cost – the cross - entropy cost
    """
    log_AL = np.nan_to_num(np.log(AL))
    mul = np.nan_to_num(np.multiply(log_AL, Y))
    mul_sum = np.nan_to_num(np.sum(mul)*(-1))
    diviation = float(1/AL.shape[1])
    return float(mul_sum*diviation)



def apply_batchnorm(A):
    """
    Performs batchnorm on the received activation values of a given layer.
    :param A: the activation values of a given layer
    :return: NA - the normalized activation values, based on the formula learned in class
    """
    miu = (np.sum(A, axis=1)) / A.shape[1]
    miu = miu.reshape(A.shape[0], 1)
    sigma = (np.sum(np.square(np.subtract(A, miu)), axis=1)) / A.shape[1]
    A_norm_up = np.subtract(A, miu)
    A_norm_down = np.sqrt(np.square(sigma)+sys.float_info.epsilon).reshape(A.shape[0], 1)
    return np.divide(A_norm_up, A_norm_down)


def Linear_backward(dZ, cache):
    """
    Implements the linear part of the backward propagation process for a single layer
    :param dZ: the gradient of the cost with respect to the linear output of the current layer (layer l)
    :param cache: tuple of values (A_prev, W, b) coming from the forward propagation in the current layer
    :return: dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
            dW -- Gradient of the cost with respect to W (current layer l), same shape as W
            db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """
    A_prev = cache.get("A")
    W = cache.get("W")
    train_size = A_prev.shape[1]
    dA_prev = np.dot(W.T, dZ)
    dW = np.dot(dZ, A_prev.T) / train_size
    db = np.sum(dZ, axis=1, keepdims=True) / train_size
    return dA_prev, dW, db


def linear_activation_backward(dA, cache, activation, dropout, dropout_cache):
    """
    Implements the backward propagation for the LINEAR->ACTIVATION layer. The function first computes dZ and then
    applies the linear_backward function.
    :param dA: post activation gradient of the current layer
    :param cache: contains both the linear cache and the activations cache
    :param activation: the activation function to be used (a string, either “softmax” or “relu”)
    :param dropout: the probability of keeping an hidden unit after the activation
    :param dropout_cache: contains mapping of the activated/dropped hidden units
    :return: dA_prev – Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
            dW – Gradient of the cost with respect to W (current layer l), same shape as W
            db – Gradient of the cost with respect to b (current layer l), same shape as b
    """
    linear_cache = cache.get("linear_cache")
    activation_cache = cache.get("activation_cache")
    if activation == 'softmax_backward':
        dZ = dA
        dA_prev, dW, db = Linear_backward(dZ, linear_cache)
    else:
        dZ = relu_backward(dA, activation_cache)
        dA_prev, dW, db = Linear_backward(dZ, linear_cache)
    if dropout < 1 and bool(dropout_cache):
        dA_prev = np.multiply(dA_prev, dropout_cache.get("cache"))
        dA_prev = np.divide(dA_prev, dropout)
    return dA_prev, dW, db


def relu_backward(dA, activation_cache):
    """
    Implements backward propagation for a ReLU unit
    :param dA: the post-activation gradient
    :param activation_cache: contains Z (stored during the forward propagation)
    :return: dZ – gradient of the cost with respect to Z
    """
    dZ = np.array(dA, copy=True)
    dZ[activation_cache < 0] = 0
    return dZ


def softmax_backward(dA, activation_cache):
    """
    Implements backward propagation for a softmax unit
    :param dA: the post-activation gradient (AL)
    :param activation_cache: contains Z (stored during the forward propagation) (used Y here for calculation)
    :return: dZ – gradient of the cost with respect to Z
    """
    dZ = dA - activation_cache
    return dZ


def L_model_backward(AL, Y, caches, dropout):
    """
    Implement the backward propagation process for the entire network.
    :param AL: the probabilities vector, the output of the forward propagation (L_model_forward)
    :param Y: the true labels vector (the "ground truth" - true classifications)
    :param caches: list of caches containing for each layer: a) the linear cache; b) the activation cache
    :param dropout: the probability of keeping an hidden unit after the activation
    :return: Grads - a dictionary with the gradients
    """
    grads = {}
    dropout_cache = {}
    layers_size = len(caches)
    dZ = softmax_backward(AL, Y)

    if dropout < 1:
        dropout_cache = {'prob': dropout, 'cache': caches[layers_size-2].get('dropout_cache')}

    dA_prev, dW, db = linear_activation_backward(dZ, caches[layers_size-1], 'softmax_backward', dropout, dropout_cache)
    grads["dA" + str(layers_size-1)] = dA_prev
    grads["dW" + str(layers_size)] = dW
    grads["db" + str(layers_size)] = db

    for layer_num in range(layers_size - 1, 0, -1):
        if dropout < 1 and layer_num - 2 > 0:
            dropout_cache = {'prob': dropout, 'cache': caches[layer_num - 2].get('dropout_cache')}
        else:
            dropout_cache = {}
        dA_prev, dW, db = linear_activation_backward(grads["dA" + str(layer_num)], caches[layer_num-1], 'relu_backward', dropout, dropout_cache)
        grads["dA" + str(layer_num-1)] = dA_prev
        grads["dW" + str(layer_num)] = dW
        grads["db" + str(layer_num)] = db
    return grads


def Update_parameters(parameters, grads, learning_rate):
    """
    Updates parameters using gradient descent
    :param parameters: a python dictionary containing the DNN architecture’s parameters
    :param grads: a python dictionary containing the gradients (generated by L_model_backward)
    :param learning_rate: the learning rate used to update the parameters (the “alpha”)
    :return: parameters – the updated values of the parameters object provided as input
    """
    max_iter = int(len(parameters) / 2)
    for layer_num in range(max_iter):
        parameters["W{}".format(layer_num+1)] -= learning_rate * grads["dW" + str(layer_num+1)]
        parameters["b{}".format(layer_num+1)] -= learning_rate * grads["db" + str(layer_num+1)]
    return parameters


def L_layer_model(X, Y, layers_dims, learning_rate, num_iterations, batch_size, batchnorm, dropout):
    """
    Implements a L-layer neural network. All layers but the last should have the ReLU activation
    function, and the final layer will apply the softmax activation function. The size of the output layer should
    be equal to the number of labels in the data. Please select a batch size that enables your code to run well (
    i.e. no memory overflows while still running relatively fast).
    Hint: the function should use the earlier functions in the following order: initialize -> L_model_forward ->
    compute_cost -> L_model_backward -> update parameters
    :param X: the input data, a numpy array of shape (height*width , number_of_examples)
                Comment: since the input is in grayscale we only have height and width, otherwise it would have been
                height*width*3
    :param Y: the “real” labels of the data, a vector of shape (num_of_classes, number of examples)
    :param layers_dims: a list containing the dimensions of each layer, including the input
    :param learning_rate: the learning rate
    :param num_iterations: the number of iterations
    :param batch_size: the number of examples in a single training batch.
    :param batchnorm: a boolean flag used to determine whether to apply batchnorm after the activation
    :param dropout: the probability of keeping an hidden unit after the activation
    :return: parameters – the parameters learnt by the system during the training (the same parameters that were
                        updated in the update_parameters function).
            costs – the values of the cost function (calculated by the compute_cost function). One value is to be saved
            after each 100 training iterations (e.g. 3000 iterations -> 30 values).
            train_accuracy - the accuracy of the network on the training set
            validation_accuracy - the accuracy of the network on the validation set
            epochs - the number of epochs needed to train the network
            iteration - the number of iterations needed to train the network
    """

    curr_epoch = 0
    costs = []
    batch_num = 0
    not_improve = 0
    best_validation_cost = np.inf
    i = 0

    np.random.seed(163)

    x_train, x_validation, y_train, y_validation = train_test_split(X.T, Y.T, test_size=0.2)
    x_train = x_train.T
    x_validation = x_validation.T
    y_train = y_train.T
    y_validation = y_validation.T

    batchs_ends = np.arange(batch_size,x_train.shape[1], batch_size)
    batches_x = np.split(x_train, batchs_ends, axis=1)
    batches_y = np.split(y_train, batchs_ends, axis=1)

    parameters = initialize_parameters(layers_dims)

    for i in range(num_iterations):
        if batch_num > (len(batches_x)-1):  # if we ended an epoch, we went through all the batches
            curr_epoch = curr_epoch + 1
            batch_num = 0
            x_train, x_validation, y_train, y_validation = train_test_split(X.T, Y.T, test_size=0.2)
            x_train = x_train.T
            x_validation = x_validation.T
            y_train = y_train.T
            y_validation = y_validation.T
            batches_x = np.split(x_train, batchs_ends, axis=1)
            batches_y = np.split(y_train, batchs_ends, axis=1)

        AL, caches = L_model_forward(batches_x[batch_num], parameters, batchnorm, dropout)
        cost = compute_cost(AL, batches_y[batch_num])
        grads = L_model_backward(AL, batches_y[batch_num], caches, dropout)
        parameters = Update_parameters(parameters, grads, learning_rate)

        if i % 100 == 0 and i != 0:
            print("The {} iteration completed".format(i))
            AL_validation, caches = L_model_forward(x_validation, parameters, batchnorm, 1)
            validation_cost = compute_cost(AL_validation, y_validation)

            if validation_cost < best_validation_cost:
                if (best_validation_cost - validation_cost) < 0.0000000000000001:
                    not_improve += 1
                else:
                    best_validation_cost = validation_cost
                    not_improve = 0
            else:
                not_improve += 1
            costs.append(cost)

        if not_improve == 100:
            break
        batch_num = batch_num + 1

    train_accuracy = Predict(x_train, y_train, parameters, batchnorm)
    validation_accuracy = Predict(x_validation, y_validation, parameters, batchnorm)
    epochs = curr_epoch + 1
    iteration = i
    return parameters, costs, train_accuracy, validation_accuracy, epochs, iteration


def Predict(X, Y, parameters, batchnorm):
    """
    The function receives an input data and the true labels and calculates the accuracy of the trained neural network
    on the data.
    :param X: the input data, a numpy array of shape (height*width, number_of_examples)
    :param Y: the “real” labels of the data, a vector of shape (num_of_classes, number of examples)
    :param parameters: a python dictionary containing the DNN architecture’s parameters
    :param batchnorm: a boolean flag used to determine whether to apply batchnorm after the activation
    :return: accuracy – the accuracy measure of the neural net on the provided data (i.e. the percentage of the
            samples for which the correct label receives the hughest confidence score). Use the softmax function to
            normalize the output values.
    """
    AL, caches = L_model_forward(X, parameters, batchnorm, 1)
    predictions = np.argmax(AL, axis=0)
    true_labels = np.argmax(Y, axis=0)
    correct = np.equal(predictions, true_labels)
    accuracy = float(np.sum(correct)/X.shape[1])
    return accuracy


def mnist_data(learning_rate, num_iterations, batch_size, batchnorm, dropout):
    """
    The function trains the network on the mnist dataset, and measures the performance, running time and the number of
    iterations and epochs needed to train the network.
    :param learning_rate: the learning rate
    :param num_iterations: the number of iterations
    :param batch_size: the number of examples in a single training batch.
    :param batchnorm: a boolean flag used to determine whether to apply batchnorm after the activation
    :param dropout: the probability of keeping an hidden unit after the activation
    :return: accuracy - list of the accuracies of the network on train, validation and test respectively
             runtime - the runtime to train and test the network on the mnist dataset
             iteration_num - the number of iterations needed to train the network
             epoch_num - the number of epochs needed to train the network
    """
    (x_train, y_train), (x_test, y_test) = mnist.load_data()
    x_train = x_train.reshape(x_train.shape[0], -1)
    x_test = x_test.reshape(x_test.shape[0], -1)
    y_train = np_utils.to_categorical(y_train, 10)
    y_test = np_utils.to_categorical(y_test, 10)

    layers_dims = np.array([x_train.shape[1], 20, 7, 5, 10])

    start_time = time.time()
    parameters, costs, train_accuracy, validation_accuracy, epoch_num, iteration_num = L_layer_model(x_train.T, y_train.T, layers_dims, learning_rate, num_iterations, batch_size, batchnorm, dropout)
    runtime = time.time() - start_time

    cost_df = pd.DataFrame(costs, columns=['costs'])
    cost_df["iter"] = range(100,(cost_df.shape[0]+1)*100,100)
    cost_df = cost_df.set_index("iter")

    test_accuracy = Predict(x_test.T, y_test.T, parameters, batchnorm)
    accuracy = [[train_accuracy, validation_accuracy, test_accuracy]]
    iterations = np.arange(1, iteration_num, 100)
    cost_df.set_index(iterations)
    cost_df.to_csv("costs.csv")
    return accuracy, runtime, iteration_num, epoch_num


def main():
    if len(sys.argv) > 1:
        learning_rate = float(sys.argv[1])
        num_iterations = int(sys.argv[2])
        batch_size = int(sys.argv[3])
        batchnorm = bool(sys.argv[4])
        dropout = float(sys.argv[5])
    else:
        learning_rate = 0.009
        num_iterations = 60000
        batch_size = 30
        batchnorm = False
        dropout = 1
    accuracy, runtime, iterations, epoch = mnist_data(learning_rate, num_iterations, batch_size, batchnorm, dropout)
    print(pd.DataFrame(accuracy, columns=["Train_Accuracy","Validation_Accuracy", "Test_Accuracy"]))
    print("runtime: {}".format(runtime))
    print("iterations: {}".format(iterations))
    print("epoch: {}".format(epoch))


if __name__ == "__main__":
    main()




