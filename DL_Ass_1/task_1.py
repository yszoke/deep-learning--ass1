import os
import time

import numpy as np
from sklearn.model_selection import train_test_split
import tensorflow as tf
import operator
import matplotlib.pyplot as plt

# define seed for random
np.random.seed(420)


# Forward propagation (Task 1)
def initialize_parameters(layer_dims):
    """
    :param layer_dims: an array of the dimensions of each layer in the network (layer 0 is the size of the flattened
    input, layer L is the output softmax)
    :return: a dictionary containing the initialized W and b parameters of each layer (W1…WL, b1…bL).
    Hint: Use the randn and zeros functions of numpy to initialize W and b, respectively
    """
    parameters = {}
    for l in range(1, len(layer_dims)):  # first value is not a layer, its the inputs
        parameters[l] = [np.random.randn(layer_dims[l], layer_dims[l - 1]) * np.sqrt(2 / layer_dims[l]),
                         np.zeros((layer_dims[l], 1))]

    return parameters


def linear_forward(A, W, b):
    """
    Implement the linear part of a layer's forward propagation.
    :param A: the activations of the previous layer
    :param W: the weight matrix of the current layer (of shape [size of current layer, size of previous layer])
    :param b: the bias vector of the current layer (of shape [size of current layer, 1])
    :return:
    Z – the linear component of the activation function (i.e., the value before applying the non-linear function)
    linear_cache – a dictionary containing A, W, b (stored for making the backpropagation easier to compute)
    Z =w * A + b
    """
    Z = np.dot(W, A)
    Z += b
    linear_cache = (A, W, b)
    return Z, linear_cache


def softmax(Z):
    """
    :param Z: the linear component of the activation function
    :return:
    A – the activations of the layer
    activation_cache – returns Z, which will be useful for the backpropagation.
    """
    # # original
    e = np.exp(Z)
    A = np.zeros((e.shape[0], e.shape[1]))
    for i in range(A.shape[0]):  # classes
        for j in range(A.shape[1]):  # examples
            A[i, j] = e[i, j] / np.sum(e[:, j])
    activation_cache = Z
    return A, activation_cache


def relu(Z):
    """
    :param Z: the linear component of the activation function
    :return:
    A – the activations of the layer
    activation_cache – returns Z, which will be useful for the backpropagation
    g(Z) = max(0,Z)
    """
    A = np.maximum(0, Z)
    activation_cache = Z
    return A, activation_cache


def linear_activation_forward(A_prev, W, B, activation):
    """
    Implement the forward propagation for the LINEAR->ACTIVATION layer
    :param A_prev: activations of the previous layer
    :param W: the weights matrix of the current layer
    :param B: the bias vector of the current layer
    :param activation: the activation function to be used (a string, either “softmax” or “relu”)
    :return:
    A – the activations of the current layer
    cache – a joint dictionary containing both linear_cache and activation_cache
    """
    A = None
    activation_cache = None

    Z, linear_cache = linear_forward(A_prev, W, B)
    if activation == "softmax":
        A, activation_cache = softmax(Z)
    elif activation == "relu":  # activation = "relu"
        A, activation_cache = relu(Z)
    cache = [linear_cache, activation_cache]
    return A, cache


def l_model_forward(X, parameters, use_batchnorm):
    """
    Implement forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SOFTMAX computation
    :param X: the data, numpy array of shape (input size, number of examples)
    :param parameters: – the initialized W and b parameters of each layer
    :param use_batchnorm: a boolean flag used to determine whether to apply batchnorm after the activation (note that
    this option needs to be set to “false” in Section 3 and “true” in Section 4).
    :return:
    AL – the last post-activation value
    caches – a list of all the cache objects generated by the linear_forward function
    """
    A = X
    L = len(parameters)

    caches = [[] for i in range(0, L)]

    # [LINEAR->RELU]*(L-1) run
    for l in range(1, L):
        # print(l)
        A_prev = A
        A, cache = linear_activation_forward(A_prev, parameters[l][0], parameters[l][1], "relu")
        if use_batchnorm:
            A = apply_batchnorm(A)
        caches[l - 1] = cache

    # LINEAR->SOFTMAX run
    AL, cache = linear_activation_forward(A, parameters[L][0], parameters[L][1], "softmax")
    caches[L - 1] = cache

    return AL, caches


def compute_cost(AL, Y):
    """
    Implement the cost function defined by equation. The requested cost function is categorical cross-entropy loss.
    :param AL: probability vector corresponding to your label predictions, shape (num_of_classes, number of examples)
    :param Y: the labels vector (i.e. the ground truth)
    :return:
    cost – the cross-entropy cost
    """
    n_classes = AL.shape[0]
    n_examples = AL.shape[1]

    cost = 0
    for ex in range(0, n_examples):
        for cl in range(0, n_classes):
            if Y[cl][ex] == 1:
                cost += 1 * np.log(AL[cl][ex])
    cost /= (-n_examples)
    return cost


def apply_batchnorm(A):
    """
    performs batchnorm on the received activation values of a given layer.
    :param A: the activation values of a given layer
    :return:
    NA - the normalized activation values, based on the formula learned in class
    # """
    sum_value = np.sum(A, axis=1, keepdims=True)
    mean = sum_value/A.shape[1]
    var = np.sum(np.square(A-mean), axis=1, keepdims=True)/A.shape[1]
    eps = np.finfo(float).eps
    NA = np.divide(np.subtract(A, mean), np.sqrt(var+eps))
    return NA


# Backward propagation (Task 2)
def linear_backward(dZ, cache):
    """
    description:
    Implements the linear part of the backward propagation process for a single layer

    Input:
    dZ – the gradient of the cost with respect to the linear output of the current layer (layer l)
    cache – tuple of values (A_prev, W, b) coming from the forward propagation in the current layer

    Output:
    dA_prev -- Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    dW -- Gradient of the cost with respect to W (current layer l), same shape as W
    db -- Gradient of the cost with respect to b (current layer l), same shape as b
    """

    n_samples = cache[0].shape[1]  # m
    n_neurons = cache[2].shape[0]

    # calculating db
    db = np.array([[np.sum(dZ[i]) / n_samples] for i in range(n_neurons)])

    # calculating dW
    A_prev = cache[0]
    dW = np.dot(dZ, A_prev.T)
    dW /= n_samples

    # calculating dA_prev
    W = cache[1]
    dA_prev = np.dot(W.T, dZ)

    return dA_prev, dW, db


def linear_activation_backward(dA, cache, activation):
    """
    Description:
    Implements the backward propagation for the LINEAR->ACTIVATION layer. The function first computes dZ and then
    applies the linear_backward function.

    Some comments:
    The derivative of ReLU is f'x={1 if x>0 0 otherwise
    The derivative of the softmax function is: pi-yi, where pi is the softmax-adjusted probability of the class and yi
    is the “ground truth” (i.e. 1 for the real class, 0 for all others)
    You should use the activations cache created earlier for the calculation of the activation derivative and the
    linear cache should be fed to the linear_backward function


    Input:
    dA – post activation gradient of the current layer
    cache – contains both the linear cache and the activations cache

    Output:
    dA_prev – Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev
    dW – Gradient of the cost with respect to W (current layer l), same shape as W
    db – Gradient of the cost with respect to b (current layer l), same shape as b
    """
    dZ = None
    linear_cache = cache[0]
    activation_cache = cache[1]
    if activation == 'relu':
        dZ = relu_backward(dA, activation_cache)
    elif activation == 'softmax':
        Y = cache[2]
        activation_cache_and_Y = [activation_cache, Y]
        dZ = softmax_backward(dA, activation_cache_and_Y)
    dA_prev, dW, db = linear_backward(dZ, linear_cache)  # cache[0] - linear cache
    return dA_prev, dW, db


def relu_backward(dA, activation_cache):
    """
    Description:
    Implements backward propagation for a ReLU unit

    Input:
    dA – the post-activation gradient
    activation_cache – contains Z (stored during the forward propagation)

    Output:
    dZ – gradient of the cost with respect to Z
    """
    dZ = np.array(dA, copy=True)
    dZ[activation_cache <= 0] = 0

    return dZ


def softmax_backward(dA, activation_cache):
    """
    Description:
    Implements backward propagation for a softmax unit

    Input:
    dA – the post-activation gradient
    activation_cache – contains Z (stored during the forward propagation)

    Output:
    dZ – gradient of the cost with respect to Z
    """
    Y = activation_cache[1]
    dZ = np.subtract(dA, Y)

    return dZ


def l_model_backward(AL, Y, caches):
    """
    Description:
    Implement the backward propagation process for the entire network.

    Some comments:
    the backpropagation for the softmax function should be done only once as only the output layers uses it and the
    RELU should be done iteratively over all the remaining layers of the network.

    Input:
    AL - the probabilities vector, the output of the forward propagation (L_model_forward)
    Y - the true labels vector (the "ground truth" - true classifications)
    Caches - list of caches containing for each layer: a) the linear cache; b) the activation cache

    Output:
    Grads - a dictionary with the gradients
                 grads["dA" + str(l)] = ...
                 grads["dW" + str(l)] = ...
                 grads["db" + str(l)] = ...
    """
    grads = {}
    L = len(caches)

    # SOFTMAX->LINEAR derivative
    last_cache_plus_Y = [i for i in caches[L - 1]]
    last_cache_plus_Y.append(Y)
    dA, dW, db = linear_activation_backward(AL, last_cache_plus_Y, "softmax")
    grads["dA" + str(L - 1)], grads["dW" + str(L)], grads["db" + str(L)] = dA, dW, db

    # [RELU->LINEAR]*(L-1) derivative
    for l in reversed(range(L - 1)):
        dA, dW, db = linear_activation_backward(grads["dA" + str(l + 1)], caches[l], "relu")
        grads["dA" + str(l)], grads["dW" + str(l + 1)], grads["db" + str(l + 1)] = dA, dW, db

    return grads


def update_parameters(parameters, grads, learning_rate):
    """
    Description:
    Updates parameters using gradient descent

    Input:
    parameters – a python dictionary containing the DNN architecture’s parameters
    grads – a python dictionary containing the gradients (generated by L_model_backward)
    learning_rate – the learning rate used to update the parameters (the “alpha”)

    Output:
    parameters – the updated values of the parameters object provided as input
    """
    for i in range(1, len(parameters) + 1):
        parameters[i][0] = parameters[i][0] - (learning_rate * grads["dW" + str(i)])
        parameters[i][1] = parameters[i][1] - (learning_rate * grads["db" + str(i)])

    return parameters


# Task 3 - use previous functions to train the model
def l_layer_model(X, Y, layers_dims, learning_rate, num_iterations, batch_size, val_x_flatten, val_y_onehot, batch_norm):
    # TODO remove val_x_flatten, val_y_onehot, batch_norm
    """
    Description:
    Implements a L-layer neural network. All layers but the last should have the ReLU activation function, and the
    final layer will apply the softmax activation function. The size of the output layer should be equal to the number
    of labels in the data. Please select a batch size that enables your code to run well (i.e. no memory overflows
    while still running relatively fast).

    Hint: the function should use the earlier functions in the following order:
    initialize -> L_model_forward -> compute_cost -> L_model_backward -> update parameters

    :param X:the input data, a numpy array of shape (height*width , number_of_examples)
    Comment: since the input is in grayscale we only have height and width, otherwise it would have been height*width*3
    :param Y:the “real” labels of the data, a vector of shape (num_of_classes, number of examples)
    :param layers_dims:a list containing the dimensions of each layer, including the input
    :param learning_rate:
    :param num_iterations:the number of examples in a single training batch.
    :param batch_size:
    :return:
    parameters - the parameters learnt by the system during the training (the same parameters that were updated in the
    update_parameters function).
    costs – the values of the cost function (calculated by the compute_cost function). One value is to be saved after
    each 100 training iterations (e.g. 3000 iterations -> 30 values).
    """
    parameters = initialize_parameters(layers_dims)

    costs = []
    costs_for_graph = [[], []]
    acc_val = 0
    acc_val_before = 0
    early_stop = False
    iteration = 0
    epoch = 0
    indices = np.arange(len(Y[0, :]))
    while iteration < num_iterations and not early_stop:
        shuffled_ind = np.copy(indices)
        np.random.shuffle(shuffled_ind)
        batch_start = 0
        batch_end = batch_size
        print(f'epoch: {epoch}')
        while batch_start < len(shuffled_ind) and not early_stop:
            if batch_end > len(shuffled_ind):
                batch_indices = shuffled_ind[batch_start:len(shuffled_ind)]
            else:
                batch_indices = shuffled_ind[batch_start:batch_end]
            batch_values = X[:, batch_indices]
            batch_labels = Y[:, batch_indices]
            # print(f'iteration: {iteration}, batch_start: {batch_start}, batch_end (exclusive): {min(batch_end, len(shuffled_ind))}, start_ind: {batch_indices[0]}, end_ind: {batch_indices[-1]}, batch_indices length: {len(batch_indices)}')
            AL, caches = l_model_forward(batch_values, parameters, batch_norm)
            if iteration % 100 == 0:
                cost = compute_cost(AL, batch_labels)
                costs.append(cost)
                costs_for_graph[0].append(iteration)
                costs_for_graph[1].append(cost)
                acc_val = predict(val_x_flatten, val_y_onehot, parameters, batch_norm)
                print(f"iteration: {iteration} , Cost: {cost}, Accuracy: {acc_val}")
                if acc_val_before > acc_val + 0.1:
                    print('early stop')
                    early_stop = True
                    continue
            grads = l_model_backward(AL, batch_labels, caches)
            parameters = update_parameters(parameters, grads, learning_rate)

            batch_start += batch_size
            batch_end += batch_size
            iteration += 1
        epoch += 1
    return parameters, costs, costs_for_graph, iteration


def predict(X, Y, parameters, use_batchnorm):
    # TODO remove use_batchnorm
    """
    Description:
    The function receives an input data and the true labels and calculates the accuracy of the trained neural network
    on the data.
    :param X: the input data, a numpy array of shape (height*width, number_of_examples)
    :param Y:the “real” labels of the data, a vector of shape (num_of_classes, number of examples)
    Parameters – a python dictionary containing the DNN architecture’s parameters
    :param parameters:the accuracy measure of the neural net on the provided data (i.e. the percentage of the samples for
    accuracy - which the correct label receives the hughest confidence score). Use the softmax function to normalize the output
    values.
    :return:
    """
    AL, caches = l_model_forward(X, parameters, use_batchnorm)
    AL = softmax(AL)[0]
    acc = 0
    for i in range(AL.shape[1]):
        tmp_AL = max(enumerate(AL[:, i]), key=operator.itemgetter(1))[0]
        tmp_Y = max(enumerate(Y[:, i]), key=operator.itemgetter(1))[0]
        if tmp_AL == tmp_Y:
            acc += 1
    return acc / Y.shape[1]


def save_run(costs, parameters, predict, batch_size, batch_norm):
    name_dic = parameters
    if not os.path.exists("output/" +name_dic):
        os.mkdir("output/" +name_dic)

    f = open("output/" + name_dic + "/results.txt", "w")
    f.write(f"Parameters: {parameters}\n")
    f.write(f"Accuracy Train: {predict[0]}\n")
    f.write(f"Accuracy Validation: {predict[1]}\n")
    f.write(f"Accuracy Test: {predict[2]}\n")
    f.write(f"Cost: {costs[1]}\n")
    f.close()

    # create plot
    plt.plot(costs[0], costs[1])
    plt.ylabel('cost')
    plt.xlabel('iterations')
    plt.title(f'Batch Size: {batch_size}, Batchnorm: {batch_norm}')
    plt.savefig("output/" + name_dic + "/plot.png")


if __name__ == "__main__":
    data = tf.keras.datasets.mnist.load_data()
    X_train, X_test, y_train, y_test = data[0][0], data[1][0], data[0][1], data[1][1]
    X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=1)
    layers = [784, 20, 7, 5, 10]

    train_x_flatten = X_train.reshape(X_train.shape[0], -1).T  # The "-1" makes reshape flatten the remaining dimensions
    test_x_flatten = X_test.reshape(X_test.shape[0], -1).T
    val_x_flatten = X_val.reshape(X_val.shape[0], -1).T

    train_x_flatten = np.divide(train_x_flatten, 255)
    test_x_flatten = np.divide(test_x_flatten, 255)
    val_x_flatten = np.divide(val_x_flatten, 255)

    train_y_onehot = np.squeeze(np.eye(y_train.max() + 1)[y_train.reshape(-1)]).T
    test_y_onehot = np.squeeze(np.eye(y_test.max() + 1)[y_test.reshape(-1)]).T
    val_y_onehot = np.squeeze(np.eye(y_val.max() + 1)[y_val.reshape(-1)]).T

    batch_size = 200
    learning_rate = 0.009
    number_iterations = 20000
    batch_norm = False
    start_time = time.time()
    params, csts, costs_for_graph, number_run = l_layer_model(train_x_flatten, train_y_onehot, layers,
                                                                         learning_rate,
                                                                         number_iterations, batch_size, val_x_flatten,
                                                                         val_y_onehot, batch_norm)

    time = (time.time() - start_time)/60
    time = "%.3f" % time
    hyper_param = "batch_" + str(batch_size) + "_learningRate_" + str(learning_rate) + "_numberIterations_" + \
                  str(number_iterations) + "_batchNorm_" + str(batch_norm) + "_NumberRun_" + str(number_run) + "_Time"\
                  + str(time)
    acc_train = predict(train_x_flatten, train_y_onehot, params, batch_norm)
    acc_val = predict(val_x_flatten, val_y_onehot, params, batch_norm)
    acc_test = predict(test_x_flatten, test_y_onehot, params, batch_norm)
    predict = [acc_train, acc_val, acc_test]
    save_run(costs_for_graph, hyper_param, predict, batch_size, batch_norm)
    print("don't worry be happy")
